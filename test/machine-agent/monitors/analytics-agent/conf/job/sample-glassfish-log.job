# The version of this job configuration file. This should not be changed by the user.
version: 2

# Optional property. Defaults to "false"
#
enabled: false

# Mandatory property.
#
# Source can be of two types:
#   1) file: This allows tailing a log file. It consists of three parameters:
#       - path: Path to the directory where files need to be tailed.
#       - nameGlob: The name/expression to tail the matching files.
#       - startAtEnd: If set to true allows tailing the file from the end.
#         Example:
#         1) path: /var/log
#            nameGlob: */*.log searches for .log files one level deep in the /var/log
#            directory (matches '/var/log/cassandra/system.log' but not
#            '/var/log/apache2/logs/error.log').
#         2) path: /var/log
#            nameGlob: **/*.log searches for .log files even in the sub-directories in
#            /var/log directory (matches both '/var/log/apache2/logs/error.log' and
#            '/var/log/cassandra/system.log').
#       On Windows, path should be provided as if on Unix environments.
#       Ex: demo/logs
#       Ex: C:/app/logs
#   2) syslog: Listens to Syslog messages on a network port. It consists of two parameters:
#       - protocol: The transport layer protocol that will be used. Currently
#         we support only tcp.
#       - port: The port at which the server will be listening to receive the
#         syslog messages. Currently, we support one port per job file, and any
#         enabled job file should be configured to receive messages at different
#         ports. The user must make sure it does not conflict with anything else
#         active in network. If no port number is provided, default port 514
#         will be used.
#
source:
    type: file
    path: ${APPDYNAMICS_HOME}/controller-tmp/glassfish/glassfish/domains/domain1/logs
    nameGlob: server.log
    startAtEnd: false

# Optional property.
#
# If provided, then it means the file being tailed has records that span
# multiple lines.
#
# If not provided then it means that each line read from the file is to
# be treated as a separate record.
#
# To identify the beginning of a record, there are 2 options:
#   1) "startsWith" followed by the first few characters of a line that
#      indicate the beginning of a record.
#   2) "regex" followed by the regular expression that indicates the
#      beginning of a record.
#
multiline:
   startsWith: "[#|"

# Optional property (Except "sourceType").
#
# These fields are in addition to the data that is already present in the
# files being tailed. Each record read from the file will be enriched with
# these fields.
#
fields:
   sourceType: glassfish-server-log
   nodeName: JEE server
   tierName: Order Processing Tier
   appName: eBook Shop

# Optional property.
#
# Grok is a way to define and use complex, nested regular expressions in an
# easy to read and use format.
#
# A Grok pattern ultimately resolves and compiles into a regular expression.
# The advantage of using Grok is its ability to compose complex patterns from
# simpler pattern definitions, like a "formal grammar".
#
# See https://grokdebug.herokuapp.com/patterns for examples.
#
# The application comes pre-loaded with some well known Grok patterns in
# the form of ".grok" files. They are available under the "conf/grok" directory.
# Custom Grok files can be added to this directory and they will be
# available for use here when the application is restarted.
#
# The Grok patterns here are meant to match a part of the log "message" string.
# If multiple Grok patterns are provided, each one will be applied to the
# "message" string individually.
#
# A Grok pattern is really a regular expression with the option of referencing
# other known Grok patterns by name. Like this "%{JAVACLASS:myClassName}".
# This means that we are looking for a sub-string that looks like a Java Class
# name. Once the pattern is found, the matching sub-string will be extracted
# and stored separately as a first class field, with "myClassName" as the key.
#
# By default, these patterns do not match multiline strings. To look for
# the pattern sub-string across a multiline string, please refer to:
# http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html#DOTALL
#

grok:
  patterns:
    - "\\[\\#\\|%{TIMESTAMP_ISO8601:eventTimestamp}\\|%{LOGLEVEL:logLevel}\\|%{DATA:serverVersion}\\|%{JAVACLASS:class}\\|%{DATA:keyValuePairs}\\|%{GREEDYDATA}"


# Optional property.
#
# The KeyValue Stage is a way to support parsing of logs to identify
# key-value pairs with a user defined delimiter.
# The KeyValue Stage is defined after the Grok Stage (if defined), and can leverage
# parsing done in the GrokStage to identify the key-value pairs in the logs.
# This stage captures the key-value pairs from fields specified by
# the "source" parameter. The values listed under the "source" parameter must
# refer to fields that were captured in stages that occurred before the invocation
# of the KeyValue stage. For example, if you had a Grok Stage which defined the
# following pattern "%{DATA:keyValuePairs}" then you could list the
# field "keyValuePairs" under the "source" parameter to capture any key-value pairs
# listed in the "keyValuePairs" string. If the "source" parameter is not specified
# then the KeyValue Stage will attempt to extract key-value pairs from the entire message.
#
# The KeyValue Stage contains the following fields:
#
#   1) "source": A list of strings on which KeyValue filter should be applied.
#      It is an optional field. If it is not provided the key-value pairs
#      are parsed from the original log "message" string.
#   2) "split": The delimiter defined by the user to separate out the
#      key from value. For example, key=value, "split: '=' ".
#   3) "separator": The delimiter defined by the user to separate out  two key-value pairs.
#      For example, key1=value1;key2=value2, "separator: ';' " and "split: '=' ".
#   4) "include": A list of key names that the user wants to capture from
#      the "source". If nothing is provided in "include" we
#      capture all the key-value pairs.
#   5) "trim": A list of characters the user wants to remove from the start and end of the key/value
#      before storing them.

keyValue:
   source:
    - "keyValuePairs"
   split: "="
   separator: ";"
   include:
    - "ThreadID"
    - "ThreadName"
   trim:
    - "_"


# Optional property.
#
# RequestGuid attempts to extract the request GUID from the specified "source" field.
# For details refer to 'sample-analytics-log-with-request-guid.job' file.

# Optional property.
#
# If records have a timestamp that should be used as "the" eventTimestamp then
# the format can be provided here to ensure that the string gets parsed and
# transformed correctly to UTC time zone.
#
# An attempt will be made to extract the timestamp automatically, failing which
# one will be added at the time the record is read from the file.
#
# UTC time zone is used throughout the system to ensure consistency of
# timestamps across sources from different time zones. This means that all
# timestamps should be converted to UTC time zone.
#
# If the format ends with a "z" or "Z" then the time zone offset is used to
# convert to UTC time. No time zone means local time zone.
#
# A reference list of available patterns can be found here:
# http://www.joda.org/joda-time/key_format.html
#
eventTimestamp:
   pattern: "yyyy-MM-dd'T'HH:mm:ss.SSSZ"


# Optional property.
#
# This stage is applied after all fields have been captured from the log message.
# The user can specify a list of field names, for which they want to cast
# the value to a specific type or rename the field with an "alias".
#
# The Transform Stage contains the following fields:
#
#   1) "field": This cannot be empty or null. It specifies the name of the
#      field to transform. If "field" is defined, either "type" or "alias"
#      must be specified. If neither is specified, an error will be thrown.
#   2) "alias": The new name which the "field" will be referred by.
#   3) "type": The value type the field will be cast to.
#      "type" can be "NUMBER", "BOOLEAN" or "STRING". By default it is "STRING".

transform:
   fields:
       - field: "ThreadID"
         type: "NUMBER"
       - field: "ThreadName"
         alias: "NewThreadName"
       - field: "ThreadID"
         type: "NUMBER"
         alias: "NewThreadID"

# ####################################
# ###   Start sample file format   ###
# ####################################
#
#[#|2015-09-24T06:33:31.574-0700|INFO|glassfish3.1.2|com.appdynamics.METRICS.WRITE|_ThreadID=206;_ThreadName=Thread-2;|NODE PURGER Completed in 14 ms|#]
#[#|2015-09-24T06:33:46.541-0700|INFO|glassfish3.1.2|com.singularity.ee.controller.beans.license.LicenseUsageManagerBean|_ThreadID=202;_ThreadName=Thread-2;|About to start persisting  license usage data |#]
#[#|2015-09-24T06:33:46.546-0700|SEVERE|glassfish3.1.2|com.singularity.ee.controller.beans.license.LicenseUsageManagerBean|_ThreadID=202;_ThreadName=Thread-2;|Unable to identify license status for module db-agent and account customer1 considering it not licensed|#]
#[#|2015-09-24T06:33:46.552-0700|SEVERE|glassfish3.1.2|com.singularity.ee.controller.beans.license.LicenseUsageManagerBean|_ThreadID=202;_ThreadName=Thread-2;|Unable to identify license status for module db-agent and account customer3 considering it not licensed|#]
#[#|2015-09-24T06:33:46.559-0700|SEVERE|glassfish3.1.2|com.singularity.ee.controller.beans.license.LicenseUsageManagerBean|_ThreadID=202;_ThreadName=Thread-2;|Unable to identify license status for module db-agent and account customer2 considering it not licensed|#]
#[#|2015-09-24T06:33:55.000-0700|INFO|glassfish3.1.2|com.appdynamics.RULES.PROCESSING|_ThreadID=201;_ThreadName=Thread-2;|Running HR processor, checking HR cache|#]
#[#|2015-09-24T06:33:55.001-0700|INFO|glassfish3.1.2|com.appdynamics.RULES.PROCESSING|_ThreadID=201;_ThreadName=Thread-2;|Running HR processor, cache loaded|#]
#
# ##################################
# ###   End sample file format   ###
# ##################################